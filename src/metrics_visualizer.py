"""
Metrics Visualization Module for Computer Vision Project
This module provides visualization tools for model evaluation results.
"""

import os
import json
import glob
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pathlib import Path
import seaborn as sns
from datetime import datetime
import re
from collections import defaultdict
import matplotlib.ticker as mticker

# Set plot style for consistent visualizations
plt.style.use('ggplot')
sns.set_style("darkgrid")
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['font.size'] = 12

# Configure paths
PROJECT_ROOT = Path(__file__).resolve().parent.parent
RESULTS_DIR = PROJECT_ROOT / "inference" / "results"
VISUALIZATIONS_DIR = RESULTS_DIR / "visualizations"
VISUALIZATIONS_DIR.mkdir(exist_ok=True, parents=True)

# Model visualization attributes
MODEL_COLORS = {
    'mask-rcnn': '#1f77b4',  # blue
    'yolo-seg': '#ff7f0e',   # orange
}

MODEL_NAMES = {
    'mask-rcnn': 'Mask R-CNN',
    'yolo-seg': 'YOLO-Seg',
}

class MetricsVisualizer:
    """Class to visualize model evaluation metrics and results"""
    
    def __init__(self, results_file=None, results_dir=None):
        """
        Initialize the visualizer with evaluation results
        
        Args:
            results_file (str or Path, optional): Path to specific results JSON file
            results_dir (str or Path, optional): Directory containing result files
        """
        self.results_dir = Path(results_dir) if results_dir else RESULTS_DIR
        self.results = self._load_latest_results(results_file)
        
    def _load_latest_results(self, specific_file=None):
        """
        Load the latest results file or a specific file if provided
        
        Args:
            specific_file (str or Path, optional): Path to specific results file
            
        Returns:
            dict: Evaluation results data
        """
        if specific_file:
            file_path = Path(specific_file)
            if not file_path.exists():
                print(f"Results file not found: {file_path}")
                return None
        else:
            # Find the most recent evaluation results file
            result_files = sorted(self.results_dir.glob("evaluation_results_*.json"))
            if not result_files:
                print(f"No evaluation results found in {self.results_dir}")
                return None
            
            file_path = result_files[-1]  # Most recent file
        
        print(f"Loading results from {file_path}")
        try:
            with open(file_path, 'r') as f:
                results = json.load(f)
            return results
        except Exception as e:
            print(f"Error loading results file: {e}")
            return None
    
    def create_metrics_dashboard(self, show_plot=False):
        """
        Create a comprehensive dashboard of model metrics
        
        Args:
            show_plot (bool): Whether to display the plot interactively
            
        Returns:
            str: Path to saved visualization or None if fails
        """
        if not self.results:
            print("No results data available")
            return None
        
        try:
            # Extract model names and metrics
            model_types = list(self.results.keys())
            
            # Set up the figure
            fig = plt.figure(figsize=(16, 12))
            fig.suptitle("Computer Vision Model Performance Dashboard", fontsize=20)
            
            # Grid layout: 3 rows x 2 columns
            gs = plt.GridSpec(3, 2, figure=fig)
            
            # Subplot 1: Speed (FPS)
            ax1 = fig.add_subplot(gs[0, 0])
            fps_values = [self.results[m].get("fps", 0) for m in model_types]
            model_names = [MODEL_NAMES.get(m, m) for m in model_types]
            colors = [MODEL_COLORS.get(m, "gray") for m in model_types]
            ax1.bar(model_names, fps_values, color=colors)
            ax1.set_title("Processing Speed (FPS)")
            ax1.set_ylabel("Frames Per Second")
            for i, v in enumerate(fps_values):
                ax1.text(i, v + 0.5, f"{v:.1f}", ha='center')
            
            # Subplot 2: Mean Average Precision
            ax2 = fig.add_subplot(gs[0, 1])
            map_values = []
            map_50_values = []
            map_75_values = []
            
            for m in model_types:
                coco_metrics = self.results[m].get("coco_metrics", {})
                map_values.append(coco_metrics.get("AP_IoU=0.50:0.95", 0))
                map_50_values.append(coco_metrics.get("AP_IoU=0.50", 0))
                map_75_values.append(coco_metrics.get("AP_IoU=0.75", 0))
            
            x = np.arange(len(model_names))
            width = 0.25
            
            ax2.bar(x - width, map_values, width, label="mAP (IoU=0.50:0.95)", color="steelblue")
            ax2.bar(x, map_50_values, width, label="AP (IoU=0.50)", color="forestgreen")
            ax2.bar(x + width, map_75_values, width, label="AP (IoU=0.75)", color="darkred")
            
            ax2.set_xticks(x)
            ax2.set_xticklabels(model_names)
            ax2.set_title("Mean Average Precision (COCO)")
            ax2.set_ylabel("AP")
            ax2.legend()
            
            # Function to add labels to bars
            def add_labels(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f"{height:.3f}", ha='center', va='bottom', fontsize=9)
            
            # Add labels on each bar group
            bars = ax2.containers
            for bar in bars:
                add_labels(bar)
            
            # Subplot 3: Detection counts
            ax3 = fig.add_subplot(gs[1, 0])
            detection_counts = [self.results[m].get("total_detections", 0) for m in model_types]
            ax3.bar(model_names, detection_counts, color=colors)
            ax3.set_title("Total Objects Detected")
            for i, v in enumerate(detection_counts):
                ax3.text(i, v + 5, f"{v}", ha='center')
            
            # Subplot 4: Class Distribution (combine top classes across models)
            ax4 = fig.add_subplot(gs[1, 1])
            
            # Collect class distribution data
            class_distribution = {}
            for m in model_types:
                detection_counts = self.results[m].get("detection_counts", {})
                for cls, count in detection_counts.items():
                    if cls not in class_distribution:
                        class_distribution[cls] = {}
                    class_distribution[cls][m] = count
            
            # Filter to top N classes by total count
            top_n = 10
            sorted_classes = sorted(class_distribution.items(), 
                                   key=lambda x: sum(x[1].values()), reverse=True)[:top_n]
            
            # Prepare data for stacked bar chart
            class_names = [name for name, _ in sorted_classes]
            model_data = {model: [] for model in model_types}
            
            for cls_name, counts in sorted_classes:
                for model in model_types:
                    model_data[model].append(counts.get(model, 0))
            
            # Plot stacked bar chart
            bottom = np.zeros(len(class_names))
            for i, model in enumerate(model_types):
                ax4.bar(class_names, model_data[model], bottom=bottom, 
                       label=MODEL_NAMES.get(model, model), color=MODEL_COLORS.get(model, "gray"))
                bottom += np.array(model_data[model])
            
            ax4.set_title("Top Class Detections by Model")
            ax4.set_xlabel("Class")
            ax4.set_ylabel("Count")
            
            # Rotate labels if necessary to prevent overlap
            plt.setp(ax4.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
            
            # Add legend
            ax4.legend(title="Model")
            
            # Subplot 5: Size-based performance
            ax5 = fig.add_subplot(gs[2, 0])
            
            size_metrics = {
                "small": [],
                "medium": [],
                "large": []
            }
            
            for m in model_types:
                coco_metrics = self.results[m].get("coco_metrics", {})
                size_metrics["small"].append(coco_metrics.get("AP_small", 0))
                size_metrics["medium"].append(coco_metrics.get("AP_medium", 0))
                size_metrics["large"].append(coco_metrics.get("AP_large", 0))
            
            # Plot grouped bar chart
            x = np.arange(len(model_names))
            width = 0.2
            
            ax5.bar(x - width, size_metrics["small"], width, label="Small Objects", color="lightskyblue")
            ax5.bar(x, size_metrics["medium"], width, label="Medium Objects", color="royalblue")
            ax5.bar(x + width, size_metrics["large"], width, label="Large Objects", color="darkblue")
            
            ax5.set_xticks(x)
            ax5.set_xticklabels(model_names)
            ax5.set_title("Performance by Object Size (AP)")
            ax5.set_ylabel("Average Precision")
            ax5.legend()
            
            # Subplot 6: Show key system information
            ax6 = fig.add_subplot(gs[2, 1])
            
            # Get system information from results
            model_device = {}
            for m in model_types:
                inference_times = self.results[m].get("inference_times", [])
                avg_infer_time = np.mean(inference_times) if inference_times else 0
                model_device[m] = self.results[m].get("model_device", "unknown")
            
            # Create system info text
            text_lines = [
                "Test Information",
                "----------------",
                f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"Models Evaluated: {len(model_types)}",
                f"Test Dataset: COCO val2017 subset",
                f"Images Processed: {self.results[model_types[0]].get('total_images', 'N/A')}",
                "",
                "Hardware Used",
                "------------",
            ]
            
            for m in model_types:
                text_lines.append(f"{MODEL_NAMES.get(m, m)}: {model_device.get(m, 'unknown')}")
            
            # Hide axes for text box
            ax6.axis('off')
            
            # Add text
            ax6.text(0.05, 0.95, '\n'.join(text_lines), 
                    transform=ax6.transAxes, fontsize=12, va='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            # Adjust layout
            plt.tight_layout(rect=[0, 0, 1, 0.95])  # Make space for the suptitle
            
            # Save and return path
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = VISUALIZATIONS_DIR / f"metrics_dashboard_{timestamp}.png"
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"Dashboard saved to: {output_path}")
            
            if show_plot:
                plt.show()
            else:
                plt.close(fig)
                
            return str(output_path)
        
        except Exception as e:
            print(f"Error creating metrics dashboard: {e}")
            import traceback
            traceback.print_exc()
            plt.close('all')
            return None

    def plot_precision_recall_curves(self, show_plot=False):
        """
        Generate precision-recall curves for each model
        
        Args:
            show_plot (bool): Whether to show the plot interactively
            
        Returns:
            str: Path to saved visualization or None if fails
        """
        if not self.results:
            print("No results data available")
            return None
        
        try:
            fig, ax = plt.subplots(figsize=(10, 8))
            
            # For each model, extract and plot PR curve data
            for model_type, metrics in self.results.items():
                # In a real implementation, this would extract actual PR curve data
                # Here we'll simulate it based on available metrics
                
                coco_metrics = metrics.get("coco_metrics", {})
                ap_50 = coco_metrics.get("AP_IoU=0.50", 0)
                ap_75 = coco_metrics.get("AP_IoU=0.75", 0)
                
                # Create a simulated PR curve based on the AP values
                # In a real implementation, you'd use actual PR data for each IoU threshold
                precision = np.linspace(1, 0, 100)
                recall = np.linspace(0, 1, 100)
                
                # Adjust curve to roughly match the AP values
                precision_adjusted = precision * ap_50 * (1 - recall**(1/2))
                
                # Plot the curve
                ax.plot(recall, precision_adjusted, 
                      label=f"{MODEL_NAMES.get(model_type, model_type)} (AP50={ap_50:.3f}, AP75={ap_75:.3f})",
                      color=MODEL_COLORS.get(model_type, "gray"),
                      linewidth=2)
            
            # Add reference line for random classifier
            ax.plot([0, 1], [0.5, 0.5], 'k--', alpha=0.3, label='Random Classifier')
            
            # Set up the plot
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('Recall')
            ax.set_ylabel('Precision')
            ax.set_title('Precision-Recall Curve')
            ax.legend(loc='lower left')
            ax.grid(True)
            
            # Save and return path
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = VISUALIZATIONS_DIR / f"precision_recall_{timestamp}.png"
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"Precision-recall curve saved to: {output_path}")
            
            if show_plot:
                plt.show()
            else:
                plt.close(fig)
                
            return str(output_path)
        
        except Exception as e:
            print(f"Error creating precision-recall curves: {e}")
            import traceback
            traceback.print_exc()
            plt.close('all')
            return None

    def plot_reliability_analysis(self, show_plot=False):
        """
        Create visualization of model reliability across multiple runs
        
        Args:
            show_plot (bool): Whether to show the plot interactively
            
        Returns:
            str: Path to saved visualization or None if fails
        """
        try:
            # Load multiple result files for reliability analysis
            result_files = sorted(self.results_dir.glob("reliability_results_*.json"))
            if not result_files:
                print(f"No reliability results found in {self.results_dir}")
                return None
            
            # Use the most recent reliability file
            file_path = result_files[-1]
            with open(file_path, 'r') as f:
                reliability_data = json.load(f)
            
            if not reliability_data:
                print("No reliability data available")
                return None
            
            # Set up the figure
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))
            
            model_types = list(reliability_data.keys())
            
            # FPS Consistency
            fps_means = [reliability_data[m].get("fps_mean", 0) for m in model_types]
            fps_stds = [reliability_data[m].get("fps_std", 0) for m in model_types]
            model_names = [MODEL_NAMES.get(m, m) for m in model_types]
            colors = [MODEL_COLORS.get(m, "gray") for m in model_types]
            
            axes[0].bar(model_names, fps_means, yerr=fps_stds, capsize=10, color=colors, alpha=0.7)
            axes[0].set_title("FPS Consistency (Mean Â± StdDev)")
            axes[0].set_ylabel("Frames Per Second")
            
            # Add CV values as text
            for i, m in enumerate(model_types):
                cv = reliability_data[m].get("fps_coef_var", 0)
                axes[0].text(i, fps_means[i] + fps_stds[i] + 0.5, f"CV: {cv:.3f}", ha='center')
            
            # Detection Count Consistency
            det_means = [reliability_data[m].get("detection_mean", 0) for m in model_types]
            det_stds = [reliability_data[m].get("detection_std", 0) for m in model_types]
            
            axes[1].bar(model_names, det_means, yerr=det_stds, capsize=10, color=colors, alpha=0.7)
            axes[1].set_title("Detection Count Consistency")
            axes[1].set_ylabel("Detected Objects Count")
            
            # Calculate coefficient of variation for detections
            for i, m in enumerate(model_types):
                mean = reliability_data[m].get("detection_mean", 0)
                std = reliability_data[m].get("detection_std", 0)
                cv = std / mean if mean > 0 else 0
                axes[1].text(i, det_means[i] + det_stds[i] + 5, f"CV: {cv:.3f}", ha='center')
            
            # Set overall title
            plt.suptitle("Model Reliability Analysis (Lower CV = More Consistent)", fontsize=16)
            plt.tight_layout(rect=[0, 0, 1, 0.95])
            
            # Save and return path
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = VISUALIZATIONS_DIR / f"reliability_analysis_{timestamp}.png"
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"Reliability analysis saved to: {output_path}")
            
            if show_plot:
                plt.show()
            else:
                plt.close(fig)
                
            return str(output_path)
        
        except Exception as e:
            print(f"Error creating reliability analysis: {e}")
            import traceback
            traceback.print_exc()
            plt.close('all')
            return None

    def load_multiple_results(self):
        """
        Load multiple result files for trend analysis
        
        Returns:
            dict: Aggregated results data over time or None if fails
        """
        try:
            # Find all evaluation result files
            result_files = sorted(self.results_dir.glob("evaluation_results_*.json"))
            if len(result_files) < 2:
                print(f"Not enough results files for trend analysis (found {len(result_files)})")
                return None
            
            # Extract date and time from filenames
            date_pattern = re.compile(r'evaluation_results_(\d{8})_(\d{6})\.json')
            
            # Structure to hold aggregated data
            aggregated_data = defaultdict(lambda: defaultdict(list))
            
            for file_path in result_files:
                # Extract timestamp from filename
                match = date_pattern.search(file_path.name)
                if not match:
                    continue
                
                date_str, time_str = match.groups()
                timestamp = datetime.strptime(f"{date_str}_{time_str}", "%Y%m%d_%H%M%S")
                
                # Load the results file
                try:
                    with open(file_path, 'r') as f:
                        results = json.load(f)
                        
                    for model_type, metrics in results.items():
                        # Extract key metrics
                        fps = metrics.get("fps", 0)
                        coco_metrics = metrics.get("coco_metrics", {})
                        map_score = coco_metrics.get("AP_IoU=0.50:0.95", 0)
                        ap50_score = coco_metrics.get("AP_IoU=0.50", 0)
                        
                        # Store in aggregated data
                        aggregated_data[model_type]["timestamps"].append(timestamp)
                        aggregated_data[model_type]["fps"].append(fps)
                        aggregated_data[model_type]["map"].append(map_score)
                        aggregated_data[model_type]["ap50"].append(ap50_score)
                        
                except Exception as e:
                    print(f"Error loading {file_path.name}: {e}")
                    continue
            
            return dict(aggregated_data)
        
        except Exception as e:
            print(f"Error loading multiple results: {e}")
            import traceback
            traceback.print_exc()
            return None

    def plot_performance_trends(self, aggregated_data=None, show_plot=False):
        """
        Plot performance trends over time
        
        Args:
            aggregated_data (dict, optional): Aggregated results data
            show_plot (bool): Whether to show the plot interactively
            
        Returns:
            str: Path to saved visualization or None if fails
        """
        if not aggregated_data:
            aggregated_data = self.load_multiple_results()
        
        if not aggregated_data:
            print("No aggregated data available for trend analysis")
            return None
        
        try:
            # Set up the figure with 3 subplots (mAP, AP50, FPS)
            fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)
            
            # Plot each model's trend
            for model_type, data in aggregated_data.items():
                timestamps = data["timestamps"]
                if not timestamps:
                    continue
                
                # Convert timestamps to numeric format for plotting
                x = np.array([(ts - timestamps[0]).total_seconds() / 3600 for ts in timestamps])
                
                # Get the display name and color
                display_name = MODEL_NAMES.get(model_type, model_type)
                color = MODEL_COLORS.get(model_type, "gray")
                
                # Plot mAP trend
                axes[0].plot(x, data["map"], 'o-', label=display_name, color=color)
                
                # Plot AP50 trend
                axes[1].plot(x, data["ap50"], 'o-', label=display_name, color=color)
                
                # Plot FPS trend
                axes[2].plot(x, data["fps"], 'o-', label=display_name, color=color)
            
            # Set titles and labels
            axes[0].set_title("Mean Average Precision (mAP) Trend")
            axes[0].set_ylabel("mAP (IoU=0.50:0.95)")
            axes[0].grid(True)
            axes[0].legend()
            
            axes[1].set_title("Average Precision at IoU=0.50 Trend")
            axes[1].set_ylabel("AP (IoU=0.50)")
            axes[1].grid(True)
            
            axes[2].set_title("Processing Speed Trend")
            axes[2].set_ylabel("FPS")
            axes[2].set_xlabel("Hours from First Evaluation")
            axes[2].grid(True)
            
            # Set overall title
            plt.suptitle("Performance Trends Over Time", fontsize=16)
            plt.tight_layout(rect=[0, 0, 1, 0.95])
            
            # Save and return path
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = VISUALIZATIONS_DIR / f"performance_trends_{timestamp}.png"
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"Performance trends saved to: {output_path}")
            
            if show_plot:
                plt.show()
            else:
                plt.close(fig)
                
            return str(output_path)
        
        except Exception as e:
            print(f"Error creating performance trends: {e}")
            import traceback
            traceback.print_exc()
            plt.close('all')
            return None

    def plot_radar_comparison(self, show_plot=False):

        if not self.results:
            print("No results data available for radar chart")
            return None
        
        try:
            model_types = list(self.results.keys())
            if not model_types:
                print("No models found in results.")
                return None

            # Define metrics for the radar chart axes
            categories = ['mAP', 'AP@50', 'AP@75', 'Norm. FPS']
            num_vars = len(categories)

            # Extract data and find max FPS for normalization
            data = []
            max_fps = 0
            for m_type in model_types:
                metrics = self.results[m_type]
                coco_metrics = metrics.get("coco_metrics", {})
                fps = metrics.get("fps", 0)
                
                if fps > max_fps:
                    max_fps = fps
                
                data.append({
                    "mAP": coco_metrics.get("AP_IoU=0.50:0.95", 0),
                    "AP@50": coco_metrics.get("AP_IoU=0.50", 0),
                    "AP@75": coco_metrics.get("AP_IoU=0.75", 0),
                    "FPS": fps # Store raw FPS first
                })

            # Prepare values for plotting (including normalized FPS)
            plot_values = []
            for i, m_type in enumerate(model_types):
                model_data = data[i]
                # Normalize FPS (avoid division by zero)
                norm_fps = (model_data["FPS"] / max_fps) if max_fps > 0 else 0
                
                values = [
                    model_data["mAP"], 
                    model_data["AP@50"], 
                    model_data["AP@75"], 
                    norm_fps
                ]
                plot_values.append(values)

            # Calculate angles for the axes
            angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
            angles += angles[:1] # Close the loop

            # Create the polar plot
            fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

            # Plot data for each model
            for i, m_type in enumerate(model_types):
                values = plot_values[i]
                values += values[:1] # Close the loop for this model's data
                
                model_name = MODEL_NAMES.get(m_type, m_type)
                color = MODEL_COLORS.get(m_type, "gray")
                
                ax.plot(angles, values, linewidth=2, linestyle='solid', label=model_name, color=color)
                ax.fill(angles, values, color, alpha=0.4)

            # Set up the axes and labels
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories)
            
            # Set y-axis ticks (adjust range if needed, e.g., if AP > 1 is possible)
            ax.set_yticks(np.arange(0, 1.1, 0.2))
            ax.set_ylim(0, 1.1) # Assuming metrics are mostly 0-1

            # Add title and legend
            plt.title("Model Comparison Radar Chart", size=16, y=1.1)
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))

            # Save and return path
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = VISUALIZATIONS_DIR / f"radar_comparison_{timestamp}.png"
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"Radar chart saved to: {output_path}")

            if show_plot:
                plt.show()
            else:
                plt.close(fig)
                
            return str(output_path)

        except Exception as e:
            print(f"Error creating radar chart: {e}")
            import traceback
            traceback.print_exc()
            plt.close('all')
            return None

    def generate_comprehensive_report(self):
        """
        Generate a comprehensive HTML report with all visualizations
        
        Returns:
            str: Path to the generated report or None if fails
        """
        try:
            # Try to import necessary modules
            try:
                from jinja2 import Template
            except ImportError:
                print("Error: jinja2 module not installed. Please install it using 'pip install jinja2'.")
                return None
            
            # Create all visualizations
            dashboard_path = self.create_metrics_dashboard(show_plot=False) or ""
            pr_curve_path = self.plot_precision_recall_curves(show_plot=False) or ""
            reliability_path = self.plot_reliability_analysis(show_plot=False) or ""
            radar_path = self.plot_radar_comparison(show_plot=False) or ""
            
            # Load aggregated data and create trends chart
            aggregated_data = self.load_multiple_results()
            trends_path = self.plot_performance_trends(aggregated_data, show_plot=False) or ""
            
            # Get basic stats
            model_types = list(self.results.keys()) if self.results else []
            
            # Create model summary table data
            model_summary = []
            for model_type in model_types:
                metrics = self.results.get(model_type, {})
                coco_metrics = metrics.get("coco_metrics", {})
                
                model_summary.append({
                    "name": MODEL_NAMES.get(model_type, model_type),
                    "fps": metrics.get("fps", 0),
                    "map": coco_metrics.get("AP_IoU=0.50:0.95", 0),
                    "ap50": coco_metrics.get("AP_IoU=0.50", 0),
                    "ap75": coco_metrics.get("AP_IoU=0.75", 0),
                    "detections": metrics.get("total_detections", 0)
                })
            
            # Get class distribution data
            class_data = []
            for model_type in model_types:
                metrics = self.results.get(model_type, {})
                detection_counts = metrics.get("detection_counts", {})
                
                # Get top 10 classes by count
                top_classes = sorted(detection_counts.items(), key=lambda x: x[1], reverse=True)[:10]
                
                # Format for template
                class_data.append({
                    "model_name": MODEL_NAMES.get(model_type, model_type),
                    "classes": [{"name": cls, "count": count} for cls, count in top_classes]
                })
            
            # HTML template using Jinja2
            template_str = """
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Computer Vision Model Evaluation Report</title>
                <style>
                    body {
                        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                        margin: 0;
                        padding: 0;
                        color: #333;
                        background-color: #f5f5f5;
                    }
                    .container {
                        max-width: 1200px;
                        margin: 0 auto;
                        padding: 20px;
                    }
                    header {
                        background-color: #2c3e50;
                        color: #ecf0f1;
                        padding: 20px;
                        text-align: center;
                        margin-bottom: 30px;
                        border-radius: 5px;
                    }
                    h1, h2, h3 {
                        font-weight: 500;
                    }
                    .chart-container {
                        background-color: white;
                        padding: 20px;
                        margin-bottom: 30px;
                        border-radius: 5px;
                        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                    }
                    .chart {
                        width: 100%;
                        text-align: center;
                    }
                    .chart img {
                        max-width: 100%;
                        height: auto;
                    }
                    table {
                        width: 100%;
                        border-collapse: collapse;
                        margin: 20px 0;
                    }
                    th {
                        background-color: #2c3e50;
                        color: white;
                        padding: 12px;
                        text-align: left;
                    }
                    tr:nth-child(even) {
                        background-color: #f2f2f2;
                    }
                    td {
                        padding: 12px;
                        border-bottom: 1px solid #ddd;
                    }
                    .model-summary {
                        margin-bottom: 30px;
                    }
                    .footer {
                        text-align: center;
                        padding: 20px;
                        margin-top: 30px;
                        font-size: 0.9em;
                        color: #7f8c8d;
                    }
                    .card {
                        background-color: white;
                        border-radius: 5px;
                        padding: 20px;
                        margin-bottom: 20px;
                        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                    }
                    .card-header {
                        border-bottom: 1px solid #eee;
                        padding-bottom: 10px;
                        margin-bottom: 20px;
                    }
                    .class-distribution {
                        display: flex;
                        flex-wrap: wrap;
                        justify-content: space-between;
                    }
                    .model-classes {
                        flex: 0 0 48%;
                        margin-bottom: 20px;
                    }
                    @media (max-width: 768px) {
                        .model-classes {
                            flex: 0 0 100%;
                        }
                    }
                </style>
            </head>
            <body>
                <header>
                    <h1>Computer Vision Model Evaluation Report</h1>
                    <p>Generated on {{ timestamp }}</p>
                </header>
                
                <div class="container">
                    <div class="card model-summary">
                        <div class="card-header">
                            <h2>Model Performance Summary</h2>
                        </div>
                        <table>
                            <thead>
                                <tr>
                                    <th>Model</th>
                                    <th>mAP</th>
                                    <th>AP@50</th>
                                    <th>AP@75</th>
                                    <th>FPS</th>
                                    <th>Total Detections</th>
                                </tr>
                            </thead>
                            <tbody>
                                {% for model in model_summary %}
                                <tr>
                                    <td>{{ model.name }}</td>
                                    <td>{{ "%.3f"|format(model.map) }}</td>
                                    <td>{{ "%.3f"|format(model.ap50) }}</td>
                                    <td>{{ "%.3f"|format(model.ap75) }}</td>
                                    <td>{{ "%.1f"|format(model.fps) }}</td>
                                    <td>{{ model.detections }}</td>
                                </tr>
                                {% endfor %}
                            </tbody>
                        </table>
                    </div>
                    
                    <div class="chart-container">
                        <h2>Performance Dashboard</h2>
                        <div class="chart">
                            <img src="{{ dashboard_path }}" alt="Performance Dashboard">
                        </div>
                    </div>
                    
                    <div class="chart-container">
                        <h2>Precision-Recall Curve</h2>
                        <div class="chart">
                            <img src="{{ pr_curve_path }}" alt="Precision-Recall Curve">
                        </div>
                    </div>
                    
                    {% if reliability_path %}
                    <div class="chart-container">
                        <h2>Reliability Analysis</h2>
                        <div class="chart">
                            <img src="{{ reliability_path }}" alt="Reliability Analysis">
                        </div>
                    </div>
                    {% endif %}
                    
                    {% if trends_path %}
                    <div class="chart-container">
                        <h2>Performance Trends</h2>
                        <div class="chart">
                            <img src="{{ trends_path }}" alt="Performance Trends">
                        </div>
                    </div>
                    {% endif %}
                    
                    <div class="card">
                        <div class="card-header">
                            <h2>Class Distribution</h2>
                        </div>
                        <div class="class-distribution">
                            {% for model in class_data %}
                            <div class="model-classes">
                                <h3>{{ model.model_name }}</h3>
                                <table>
                                    <thead>
                                        <tr>
                                            <th>Class</th>
                                            <th>Count</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        {% for class in model.classes %}
                                        <tr>
                                            <td>{{ class.name }}</td>
                                            <td>{{ class.count }}</td>
                                        </tr>
                                        {% endfor %}
                                    </tbody>
                                </table>
                            </div>
                            {% endfor %}
                        </div>
                    </div>
                    
                    <div class="card">
                        <div class="card-header">
                            <h2>Conclusion</h2>
                        </div>
                        <p>This report presents a comprehensive evaluation of two object detection and segmentation models: 
                        Mask R-CNN and YOLO-Seg. The evaluation was conducted using the COCO val2017 dataset and metrics.</p>
                        
                        <p>Based on the results, the following conclusions can be drawn:</p>
                        <ul>
                            <li>Speed: YOLO-Seg generally delivers faster inference times.</li>
                            <li>Accuracy: Mask R-CNN typically offers higher precision, particularly for complex scenes.</li>
                            <li>Trade-offs: There's a clear speed-accuracy trade-off between the models.</li>
                        </ul>
                        
                        <p>For applications requiring real-time performance, YOLO-Seg may be the preferred choice.
                        For applications where accuracy is paramount, especially for instance segmentation, 
                        Mask R-CNN may be more appropriate.</p>
                    </div>
                </div>
                
                <div class="footer">
                    <p>Generated by MetricsVisualizer | Computer Vision Project | {{ timestamp }}</p>
                </div>
            </body>
            </html>
            """
            
            # Create the template and render the HTML
            template = Template(template_str)
            html = template.render(
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                dashboard_path=dashboard_path,
                pr_curve_path=pr_curve_path,
                reliability_path=reliability_path,
                trends_path=trends_path,
                radar_path=radar_path,
                model_summary=model_summary,
                class_data=class_data
            )
            
            # Save the HTML report
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = VISUALIZATIONS_DIR / f"evaluation_report_{timestamp}.html"
            
            with open(report_path, 'w') as f:
                f.write(html)
            
            print(f"Comprehensive report generated: {report_path}")
            return str(report_path)
        
        except Exception as e:
            print(f"Error generating report: {e}")
            import traceback
            traceback.print_exc()
            return None